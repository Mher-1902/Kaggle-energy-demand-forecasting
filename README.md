# âš¡ Energy Demand Forecasting â€” PJME Hourly (Kaggle)

A complete, productionâ€‘style endâ€‘toâ€‘end time series forecasting system built using:

- **XGBoost**
- **LSTM & RNN (Neural Networks)**
- **SARIMA**
- **Prophet**
- **CUPED variance reduction**
- **Custom feature engineering**
- **Full model comparison pipeline**

All models are trained, evaluated, and compared on the **PJME Hourly Energy Consumption** dataset from Kaggle.

---

# ğŸ“Œ Key Features

### âœ” Professional project structure  
### âœ” CUPED variance reduction  
### âœ” Heavy feature engineering for ML  
### âœ” Deep learning forecasting (3 weeks â 1 week)  
### âœ” SARIMA with stationarity testing (ADF test)  
### âœ” Unified evaluation  
### âœ” Explainability (feature importance)  
### âœ” Ready for GitHub portfolio  

---

# ğŸ— Project Structure

```
â”œâ”€â”€ config
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ processed
â”‚   â”‚   â”œâ”€â”€ feature_columns.json
â”‚   â”‚   â”œâ”€â”€ pjme_cuped.csv
â”‚   â”‚   â”œâ”€â”€ xgb_features_full.csv
â”‚   â”‚   â”œâ”€â”€ X_test.npy
â”‚   â”‚   â”œâ”€â”€ X_train.npy
â”‚   â”‚   â”œâ”€â”€ X_valid.npy
â”‚   â”‚   â”œâ”€â”€ y_test.npy
â”‚   â”‚   â”œâ”€â”€ y_train.npy
â”‚   â”‚   â””â”€â”€ y_valid.npy
â”‚   â””â”€â”€ raw
â”‚       â””â”€â”€ PJME_hourly.csv
â”œâ”€â”€ environment.yml
â”œâ”€â”€ notebooks
â”‚   â”œâ”€â”€ 1_EDA.ipynb
â”‚   â”œâ”€â”€ 2_CUPED_Transformation.ipynb
â”‚   â”œâ”€â”€ 3_SARIMA_ARIMA.ipynb
â”‚   â”œâ”€â”€ 4_Prophet.ipynb
â”‚   â”œâ”€â”€ 5_RNN.ipynb
â”‚   â”œâ”€â”€ 6_LSTM.ipynb
â”‚   â”œâ”€â”€ 7_XGBoost_Feature_Engineering.ipynb
â”‚   â””â”€â”€ 8_XGBoost_Modeling.ipynb
â”œâ”€â”€ README.md
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ cuped.py
â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”œâ”€â”€ features
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ save_xgb_data.py
â”‚   â”‚   â””â”€â”€ time_features.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ lstm_model.py
â”‚   â”‚   â”œâ”€â”€ prophet_model.py
â”‚   â”‚   â”œâ”€â”€ rnn_model.py
â”‚   â”‚   â”œâ”€â”€ sarima_model.py
â”‚   â”‚   â””â”€â”€ xgboost_model.py
â”‚   â””â”€â”€ __pycache__
â”‚       â”œâ”€â”€ cuped.cpython-311.pyc
â”‚       â””â”€â”€ __init__.cpython-311.pyc
â””â”€â”€ utils
    â”œâ”€â”€ data_loader.py
    â”œâ”€â”€ make_cuped_dataset.py
    â”œâ”€â”€ plotting.py
    â””â”€â”€ __pycache__
        â”œâ”€â”€ data_loader.cpython-311.pyc
        â””â”€â”€ __init__.cpython-311.pyc

16 directories, 43 files

---

# ğŸ§  Methodology

## â­ 1. Data Pipeline  
- Loaded from Kaggle PJME hourly dataset  
- Timestampâ€‘indexed  
- Cleaned & validated  
- Split: Train / Valid / Test  

## â­ 2. CUPED Variance Reduction  
Used lagâ€‘168 values as covariate to reduce noise & stabilize model training, especially XGBoost and NN models.

## â­ 3. XGBoost Feature Engineering  
Created over 50+ predictive features:

- Hour / weekday / month / season  
- Lagged targets  
- Rolling means / std / min / max  
- Firstâ€‘order differences  
- Fourier features for yearly & weekly seasonality  


```

## â­ 4. SARIMA (Classic Time Series)
- Stationarity verified using **Dickeyâ€“Fuller test**  
- Seasonal differencing  
- AICâ€‘based hyperparameter search  
- Captures long-term seasonality + trend  

## â­ 5. Neural Networks â€” RNN & LSTM
Sliding window supervised learning:

- **Input:** 3 weeks (504 hours)  
- **Output:** 1 week (168-hour forecast)  
- Batch training  
- Early stopping  
- LSTM captures long-range dependencies  

## â­ 6. XGBoost â€” Best Model  
Boosted decision trees trained on engineered features.  
Handles non-linear behaviours, peaks, drops, and long seasonal structure extremely well.

---

### ğŸ” MLflow Experiment Tracking

This project uses **MLflow** to track all experiment runs.

Below is an example of the MLflow UI used in this project:

<p align="center">
  <img src="docs/mlflow/mlflow_ui.png" alt="MLflow UI" width="700">
</p>




# ğŸ“Š Model Performance

| Model      | RMSE | MAE |
|-----------|------:|------:|
| **XGBoost** | **231.5** | **172.3** |
| **LSTM** | 235.1 | 175.0 |
| **SARIMA** | 249.8 | 184.6 |
| **RNN** | 258.2 | 191.3 |

**Interpretation:**  
- XGBoost wins â€” but **LSTM is extremely close**.  
- SARIMA is solid classical baseline.  
- RNN is stable but less expressive.  




# ğŸŒ Badges

<p align="left">
  <img src="https://img.shields.io/badge/Python-3.11-blue">
  <img src="https://img.shields.io/badge/Framework-TensorFlow-green">
  <img src="https://img.shields.io/badge/Model-XGBoost-orange">
  <img src="https://img.shields.io/badge/License-MIT-lightgrey">
  <img src="https://img.shields.io/badge/TimeSeries-Forecasting-yellow">
</p>

---

# ğŸš€ Reproducibility

## Create environment
```bash
conda env create -f environment.yml
conda activate ts-energy-forecasting
```

## Generate ML features
```bash
python src/features/save_xgb_data.py
```

## Train models
```bash
python src/models/xgboost_model.py
python src/models/lstm_model.py
python src/models/rnn_model.py
python src/models/sarima_model.py
```



# ğŸ‘¨â€ğŸ’» Author

**Mher Sukiasyan**  
Time Series â€¢ Machine Learning â€¢ Deep Learning â€¢ Python  

---

# ğŸ“œ License

MIT License  
Free for personal and commercial use.  
